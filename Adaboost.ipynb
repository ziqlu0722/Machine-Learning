{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Adaboost.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziqlu0722/Machine-Learning/blob/master/Adaboost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "Nvz5YjprGZTH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1.Concept - Adaboost"
      ]
    },
    {
      "metadata": {
        "id": "c2YzoPxQO7_s",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Reference:\n",
        "* [ML Lecture by Hung-yi Lee](https://www.youtube.com/watch?v=tH9FH1DH5n0)\n",
        "* [AdaBoost Tutorial by Chris McCormick](http://mccormickml.com/2013/12/13/adaboost-tutorial/)"
      ]
    },
    {
      "metadata": {
        "id": "eL-36tTeWFU9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.1 Basics\n",
        "\n",
        "####Ensemble####\n",
        "\n",
        "* A complex model tends to have large variance with small bias\n",
        "\n",
        "* A simple model tends to have large bias but small variance\n",
        "\n",
        "* Multiple weak classifier ensembled could help lower the variance and improve accuracy\n",
        "  * Bagging, Boosting...\n",
        "  \n",
        "\n",
        "####Boosting####\n",
        "\n",
        "\n",
        " \n",
        " * Guarantee:\n",
        "   * If the learner can get over 50% accuracy\n",
        "   * you can obtain 0% error rate after boosting\n",
        " \n",
        " * Framework: \n",
        "   * classifiers are learned sequentially\n",
        "   * obtain the first classifier $F_1(x)$\n",
        "   * find another classifier $F_2(x)$ to help\n",
        "   * $F_1(x)$ need to be complementary with $F_2(x)$\n",
        "   * $\\dots$\n",
        "\n",
        " \n",
        "*Use bagging when model is complex, easy to overfit e.g. Random Forest for Decision Tree*\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        " "
      ]
    },
    {
      "metadata": {
        "id": "0f8BFGOKYqgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.2 Re-weight To Obtain A New Classifier\n",
        "\n",
        "* Idea: training $F_2(x)$ on the new training set that **FAILS** $F_1(x)$\n",
        "\n",
        "* How:\n",
        "\n",
        "  * $\\epsilon_1$: the error rate of $F_1(x)$ on its training data\n",
        "  \n",
        " \\begin{align}\\epsilon_1 = \\frac{\\sum\\omega_1^i \\delta~ (F_1(x^i) \\ne \\hat{y}^i)}{Z_1} = \\frac {\\sum_{F_1(x^i) \\ne \\hat{y}^i}\\omega_1^i}{Z_1}\\end{align}\n",
        " \n",
        " where $Z_1 = \\sum \\omega_1^i$\n",
        " \n",
        " * change the weights from $\\omega_1^i$ to $\\omega_2^i$ such that:\n",
        " \n",
        "    i.e. the performance of $F_1$for new weights is random\n",
        " \n",
        "\\begin{align}\\frac{\\sum\\omega_2^i \\delta~ (F_1(x^i) \\ne \\hat{y}^i)}{Z_2}= 0.5\\end{align} \n",
        "\n",
        "  * if $x^i$ misclassified by $F_{t}(x)$, we want to scale up $x^i$'s corresponding error weight\n",
        "\n",
        "    * $\\omega_{t}^{(x_i)}$ multiply $d_t$ -> $\\omega_{t+1}^{(x_i)}$\n",
        "\n",
        "  * if $x^i$ correctly classified by $F_{t}(x)$, we want to scale down $x^i$'s corresponding error weight\n",
        "\n",
        "    * $\\omega_{t}^{(x_i)}$ divide $d_t$ -> $\\omega_{t+1}^{(x_i)}$\n",
        "  \n",
        " \\begin{align}\\epsilon_{t^{(w_{t+1})}} =\n",
        "\\frac{\\sum_{F_t(x^i) \\ne \\hat{y}^i}\\omega_t^{(x_i)}~d_t}{\\sum_{F_t(x^i) \\ne \\hat{y}^i}\\omega_t^{(x_i)}~d_t + \\sum_{F_t(x^i) = \\hat{y}^i}\\omega_t^{(x_i)}/d_t} = 0.5\\end{align} \n",
        "\n",
        "\\begin{align}\\therefore \\sum_{F_t(x^i) = \\hat{y}^i}\\omega_1^{(x_i)}/d_t = \\sum_{F_t(x^i) \\ne \\hat{y}^i}\\omega_1^{(x_i)}d_t a\\end{align} \n",
        "\n",
        "\\begin{align}\\therefore Z_{t+1}(1-\\epsilon_t)/d_t = Z_{t+1}\\epsilon\\end{align} \n",
        "\n",
        "\\begin{align}\\therefore d_t = \\sqrt{\\frac{1-\\epsilon_t}{\\epsilon_t}}  = e^{ln \\sqrt{\\frac{1-\\epsilon_t}{\\epsilon_t}}}\\end{align} \n",
        "\n",
        "let \\begin{align}\\alpha_t = ln~\\sqrt{\\frac{1-\\epsilon_t}{\\epsilon_t}}\\end{align}\n",
        "  * if $x^i$ misclassified by $F_{t}(x)$,\n",
        "\n",
        "    * $\\omega_{t+1}^{(x_i} = \\omega_{t}^{(x_i)} e^{\\alpha_t}$\n",
        "\n",
        "  * else,\n",
        "\n",
        "    * $\\omega_{t+1}^{(x_i} = \\omega_{t}^{(x_i)} e^{-\\alpha_t}$\n",
        "    \n",
        "therefore, \\begin{align} \\omega_t^{(x_i)} e^{(-\\hat{y}^i F_t(x^i)\\alpha_t)} = \\omega_{t+1}^{(x_i)}\\end{align}\n",
        "\n",
        "\n",
        "Finally we get the update factor:\n",
        "\n",
        "\\begin{align} e^{(-\\hat{y}^i F_t(x^i)\\alpha_t)} \\end{align}"
      ]
    },
    {
      "metadata": {
        "id": "6Q2TxF0AXAqh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.3. Ensemble Classifiers Using $\\alpha_t$\n",
        "\n",
        "**Use $\\alpha_t$ to linearly combine all classifiers**\n",
        "\n",
        "\\begin{align}H_{(x)} = sign(\\sum_{t=1}^T\\alpha_tF_t(x))\\end{align}\n",
        "\n",
        "\\begin{align}\\alpha_t = ln~\\sqrt{\\frac{(1-\\epsilon_t)}{\\epsilon_t}}\\end{align}\n",
        "\n",
        "* Intuition: classifiers with smaller error rate gets larger importance\n",
        "\n",
        "* Larger $\\alpha_t$, smaller error $\\epsilon$, larger weight for final voting\n",
        "\n",
        "  *As we will use classifier whose error rate is larger than 0.5, $\\alpha_t$ will be positive*\n",
        "\n",
        "![](http://chrisjmccormick.files.wordpress.com/2013/12/adaboost_alphacurve.png)\n",
        "\n",
        "**3 Key Points From Above Figure**\n",
        "\n",
        "> * The classifier weight grows exponentially as the error approaches 0. Better classifiers are given exponentially more weight.\n",
        "\n",
        "> * The classifier weight is zero if the error rate is 0.5. A classifier with 50% accuracy is no better than random guessing, so we ignore it.\n",
        "\n",
        "> * The classifier weight grows exponentially negative as the error approaches 1. We give a negative weight to classifiers with worse worse than 50% accuracy. “Whatever that classifier says, do the opposite!”.\n",
        "\n",
        "\n",
        "from [source](http://mccormickml.com/2013/12/13/adaboost-tutorial/)"
      ]
    },
    {
      "metadata": {
        "id": "EJhbmK7dWMA8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1.4. $\\alpha$ Will Allow A Converge\n",
        "\n",
        "* Final Classifier: \n",
        "\\begin{align}H_{(x)} = sign(\\sum_{t=1}^T\\alpha_tF_t(x))\\end{align}\n",
        "\n",
        "\\begin{align}\\alpha_t = ln~\\sqrt{\\frac{(1-\\epsilon_t)}{\\epsilon_t}}\\end{align}\n",
        "\n",
        "let $\\sum_{t=1}^T\\alpha_tF_t(x) = g(x)$,\n",
        "\n",
        "Training Data Error Rate\n",
        "\\begin{align} =&\\frac{1}{N}\\sum_{i}\\delta~(H(x^i)\\ne \\hat{y}^i) \\\\=& \\frac{1}{N} \\sum_{i}\\delta~(-\\hat{y}^ig(x^i)< 0)\\\\\\le&\\frac{1}{N} \\sum_{i}exp~(-\\hat{y}^ig(x^i))\\end{align}\n",
        "\n",
        "becase $exp~(-\\hat{y}^ig(x^i))$ is the upper bound for $\\delta~(-\\hat{y}^ig(x^i)< 0)$\n",
        "\n",
        "<img src=\"https://alliance.seas.upenn.edu/~cis520/dynamic/2018/wiki/uploads/Lectures/exp_vs_01.png\" width=\"500\">\n",
        "\n",
        "$Z_t$: the summation of the weights of training data when training $F_t$\n",
        "\n",
        "\\begin{align}Z_{t+1} = \\sum_i \\omega_{t+1}^i \n",
        "= &\\sum_i\\omega_1^i  \\prod_{t=1}^Texp~(-\\hat{y}^iF_t(x^i)\\alpha_t) \\\\=&\\sum_i\\prod_{t=1}^Texp~(-\\hat{y}^iF_t(x^i)\\alpha_t)\\\\=&\\sum_i \\exp~(-\\hat{y}^i\\sum_{t=1}^T F_t(x^i)\\alpha_t)\\\\=&\\sum exp~(-\\hat{y}^i\\sum_{t+1}^Tg(x^i))\\end{align}\n",
        "\n",
        "$\\therefore$ error_rate =\\begin{align}\\frac{1}{N} \\sum_{i}exp~(-\\hat{y}^ig(x^i)) = \\frac{1}{N} Z_{t+1}\\end{align}\n",
        "\n",
        "\n",
        "if weights get smaller and smaller, i.e. upper bound gets smaller and smaller, so error rate gets smaller and smaller\n",
        "\n",
        "\\begin{align} Z_{t+1} = &Z_{t}\\epsilon_{t} exp~(\\alpha_{t}) + Z_{t}(1- \\epsilon_{t}) exp~(\\alpha_{t}) \\\\= &Z_{t}*2 \\sqrt{{\\epsilon_{t}}(1-\\epsilon_{t})} \n",
        "\\\\= &N * \\prod_{t=1}^T2 \\sqrt{{\\epsilon_{t}}(1-\\epsilon_{t})}\\end{align}\n",
        "\n",
        "$\\therefore$ training error rate $\\le 2 \\sqrt{{\\epsilon_{t}}(1-\\epsilon_{t})} $ and gets smaller and smaller, \n",
        "\n",
        "because $\\epsilon < 0.5$,  i.e. $2 \\sqrt{{\\epsilon_{t}}(1-\\epsilon_{t})} < 1$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ed1TR2-PuQdy",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#2. Implementation"
      ]
    },
    {
      "metadata": {
        "id": "IazMog18uX6F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Pseudocode\n",
        "\n",
        "> With:\n",
        "> * Samples $x_1 \\dots x_n$\n",
        "> * Desired outputs $y_1 \\dots y_n, y \\in \\{-1, 1\\}$\n",
        "> * Initial weights $w_{1,1} \\dots w_{n,1}$ set to $\\frac{1}{n}$\n",
        "> * Error function $E(f(x), y, i) = e^{-y_i f(x_i)}$\n",
        "> * Weak learners $h\\colon x \\rightarrow \\{-1, 1\\}$\n",
        "\n",
        "> For $t$ in $1 \\dots T$:\n",
        "> * Choose $h_t(x)$:\n",
        ">  * Find weak learner $h_t(x)$ that minimizes $\\epsilon_t$, the weighted sum error for misclassified points $\\epsilon_t = \\sum^n_{\\stackrel{i=1}{h_t(x_i)\\neq y_i}} w_{i,t} $\n",
        ">  * Choose $\\alpha_t = \\frac{1}{2} \\ln \\left(\\frac{1-\\epsilon_t}{\\epsilon_t}\\right)$\n",
        "> * Add to ensemble:\n",
        ">  * $F_t(x) = F_{t-1}(x) + \\alpha_t h_t(x)$\n",
        "> * Update weights:\n",
        ">  * $w_{i,t+1} = w_{i,t} e^{-y_i \\alpha_t h_t(x_i)}$ for all i\n",
        ">  * Renormalize $w_{i,t+1}$ such that $\\sum_i w_{i,t+1} = 1$\n",
        "\n",
        "*from [wikipedia](https://en.wikipedia.org/wiki/AdaBoost)*\n"
      ]
    },
    {
      "metadata": {
        "id": "jaSKIsaD65bH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.2. Import and Split Data"
      ]
    },
    {
      "metadata": {
        "id": "wrv0eZECufQQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import math\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nR2EkYPiXUEH",
        "colab_type": "code",
        "outputId": "4fd722ab-8470-4ded-b26f-09a7ad3a04f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('drive/Data/spambase/spambase.txt') as file:\n",
        "  for line in file:\n",
        "    line_split = line.strip().split(',')\n",
        "    d = [float(c) for c in line_split]\n",
        "    if d[-1] == 0.:\n",
        "      d[-1] = -1. \n",
        "    data.append(d)    \n",
        "    \n",
        "print('There are {} data instances'. format(len(data))) \n",
        "print(collections.Counter(np.array(data)[:,-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 4601 data instances\n",
            "Counter({-1.0: 2788, 1.0: 1813})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5cUDDhKOXunt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, folds=5):\n",
        "\n",
        "  dataset_split = []     \n",
        "  dataset_copy = list(dataset)\n",
        "  fold_size = int(len(dataset) / folds)\n",
        "  for i in range(folds):\n",
        "    fold = []\n",
        "    while len(fold) < fold_size:\n",
        "      index = randrange(len(dataset_copy))\n",
        "      fold.append(dataset_copy.pop(index))\n",
        "    dataset_split.append(fold)\n",
        "\n",
        "  idx = 0\n",
        "  test_set = []\n",
        "  train_set = []\n",
        "  for i in range(folds):\n",
        "    test_set.append(dataset_split[idx])\n",
        "    train_set_j = []\n",
        "    train_set_k = []\n",
        "    for j in dataset_split[:idx] + dataset_split[idx + 1:]:\n",
        "      for k in j:\n",
        "        train_set_k.append(k)\n",
        "    train_set.append(train_set_k)\n",
        "    idx += 1\n",
        "  \n",
        "  return train_set, test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UyqdprzKXx3E",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split data\n",
        "\n",
        "num_folds = 10\n",
        "train_set, test_set = cross_validation_split(data, folds = num_folds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_b8Unl-w7E3i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.3 Build Optimal Decision Stump As the Weak Learner"
      ]
    },
    {
      "metadata": {
        "id": "_aeldgKcLVY9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class DecisionStump:\n",
        "  \"\"\"This is for find the optimal stump\n",
        "     for each feature, each value in the feature, create a stump and its corresponding error map based on given weight for each data point\n",
        "     return the optimal stump with its configuration and error map  \n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    # The optimal stump\n",
        "    self.best_stump = {}\n",
        "    self.error = np.inf\n",
        "    self.alpha = None\n",
        "          \n",
        "  def _classify(self, X, feature_idx, threshold, polarity):\n",
        "    # Set all predictions to '1' initially\n",
        "    prediction = np.ones((len(X)))\n",
        "    \n",
        "    # The indexes where the sample values are below threshold\n",
        "    negative_idx = (polarity * X[:, feature_idx] < polarity * threshold)\n",
        "    # Label those as '-1'\n",
        "    prediction[negative_idx] = -1\n",
        "    \n",
        "    return prediction \n",
        "       \n",
        "  def build_stump(self, X, label, D):\n",
        "    \"\"\"\n",
        "    input: \n",
        "      data is array-like input\n",
        "      D is the error weight \n",
        "    \"\"\"\n",
        "    n_samples, n_features = np.shape(X)\n",
        "    self.D = D\n",
        "    self.estimate = np.zeros((n_samples, 1))\n",
        "   \n",
        "    # iterate over all features:\n",
        "    for feature_idx in range(n_features):\n",
        "      feature_values = np.expand_dims(X[:, feature_idx], axis=1)\n",
        "      unique_values = np.unique(feature_values)\n",
        "      # Try every unique feature value as threshold\n",
        "      \n",
        "      for val in unique_values:\n",
        "        polarity = 1\n",
        "        predict = self._classify(X, feature_idx, val, polarity)\n",
        "        # Set a boolean matrix to indicate if current instance is an error\n",
        "        # initially set all element to '1', i.e. all are errors\n",
        "        error_mat = np.ones((n_samples))\n",
        "        # if correctly predicted, change the element to '0':\n",
        "        error_mat[predict == label] = 0\n",
        "        # Error = sum of weights of misclassified samples\n",
        "        w_error = (D.T @ error_mat).astype(np.float)\n",
        "        \n",
        "        # If the error is over 0.5 we flip\n",
        "        if w_error > 0.5:\n",
        "          w_error = 1 - w_error\n",
        "          # reclassify given polarity is changed\n",
        "          polarity = -1\n",
        "          predict = self._classify(X, feature_idx, val, polarity)\n",
        "        \n",
        "        if w_error and w_error < self.error:\n",
        "          self.error = w_error\n",
        "          self.estimate = predict.copy()\n",
        "          self.best_stump['feature'] = feature_idx\n",
        "          self.best_stump['polarity'] = polarity\n",
        "          self.best_stump['threshold'] = val\n",
        "          # Calculate the alpha which is used to update the sample weights\n",
        "          self.alpha = 0.5 * np.log((1.0 - self.error) / (self.error + 1e-16))\n",
        "    return self\n",
        "    \n",
        "  def predict(self, X):\n",
        "    # use the optimal stump to predict\n",
        "    pred = self._classify(X, self.best_stump['feature'], self.best_stump['threshold'], self.best_stump['polarity'])\n",
        "    \n",
        "    return pred"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NUsXUzJi7PTJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.4 Build Adaboost Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "4YNlJVz4jSvk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Adaboost:\n",
        "  def __init__(self, n_clf):\n",
        "    self.n_clf = n_clf\n",
        "    self.clfs = []\n",
        "\n",
        "  def _init_param(self):\n",
        "    self.X = np.array(self.data)[:,:-1]\n",
        "    self.label = np.array(self.data)[:,-1]\n",
        "    self.n_samples, self.n_features = np.shape(self.X)\n",
        "    self.D = np.full(self.n_samples, (1 / self.n_samples))     # Initialize weights to 1/ n_samples\n",
        "    self.ensemble_est = np.zeros((self.n_samples))\n",
        "    return self\n",
        "     \n",
        "  def _ensemble(self):\n",
        "    \"\"\"Linearly combines all weak learners\n",
        "       Input: list of weak learners\n",
        "    \"\"\"\n",
        "    self.ensemble_est = np.zeros((self.n_samples))\n",
        "    for clf in self.clfs:\n",
        "      self.ensemble_est += clf.alpha * clf.estimate\n",
        "      \n",
        "    self.ensemble_err = np.count_nonzero(np.sign(self.ensemble_est) != self.label)\n",
        "    self.ensemble_err_rate = self.ensemble_err / self.n_samples\n",
        "    self.train_acc = 1 - self.ensemble_err_rate\n",
        "    self.val_acc = self.evaluate(self.test_data)\n",
        "    print('ensemble---train_acc---{:.4f}---val_acc---{:.4f}'.format(self.train_acc, self.val_acc))\n",
        "    \n",
        "    return self\n",
        "    \n",
        "  def train(self, train_data, test_data):\n",
        "    # Initiate parameters\n",
        "    self.data = train_data\n",
        "    self.test_data = test_data\n",
        "    self._init_param()\n",
        "    # Iterate through classifiers\n",
        "    for i in range(self.n_clf):\n",
        "      # build weaklearner\n",
        "      clf = DecisionStump()\n",
        "      clf.build_stump(self.X, self.label, self.D)\n",
        "      # Update error weights and normalize\n",
        "      self.D *= np.exp(-1 * clf.alpha * self.label * clf.estimate)\n",
        "      self.D /= np.sum(self.D)\n",
        "      \n",
        "      # save classifier\n",
        "      self.clfs.append(clf)\n",
        "    self._ensemble()\n",
        "\n",
        "    return self\n",
        "  \n",
        "  def evaluate(self, test_data):\n",
        "    n_samples = len(test_data)\n",
        "    X = np.array(test_data)[:,:-1]\n",
        "    label = np.array(test_data)[:,-1]\n",
        "    ensemble_est = np.zeros(n_samples)\n",
        "    for clf in self.clfs:\n",
        "      est = clf.predict(X)\n",
        "      ensemble_est += clf.alpha * est\n",
        "      \n",
        "    err = np.count_nonzero(np.sign(ensemble_est) != label)\n",
        "    err_rate = err / n_samples\n",
        "    acc = 1 - err_rate\n",
        "    \n",
        "    return acc\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uckk7Lt_7aw6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.5 Train"
      ]
    },
    {
      "metadata": {
        "id": "RrSr6_BDq-im",
        "colab_type": "code",
        "outputId": "53997e20-a413-42d6-815b-2b5f6841541c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# set hyper parameters\n",
        "n_clf = 30\n",
        "\n",
        "# train\n",
        "avg_acc = 0\n",
        "for i in range(num_folds):\n",
        "  ab = Adaboost(n_clf = n_clf).train(train_set[i], test_set[i])\n",
        "  avg_acc += ab.val_acc\n",
        "print('The Accuracy Over 10 folds Cross Validation is {}'.format(avg_acc/num_folds))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ensemble---train_acc---0.9408---val_acc---0.9152\n",
            "ensemble---train_acc---0.9384---val_acc---0.9261\n",
            "ensemble---train_acc---0.9432---val_acc---0.9413\n",
            "ensemble---train_acc---0.9384---val_acc---0.9457\n",
            "ensemble---train_acc---0.9389---val_acc---0.9457\n",
            "ensemble---train_acc---0.9382---val_acc---0.9370\n",
            "ensemble---train_acc---0.9372---val_acc---0.9283\n",
            "ensemble---train_acc---0.9399---val_acc---0.9435\n",
            "ensemble---train_acc---0.9408---val_acc---0.9261\n",
            "ensemble---train_acc---0.9389---val_acc---0.9304\n",
            "The Accuracy Over 10 folds Cross Validation is 0.9339130434782609\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}