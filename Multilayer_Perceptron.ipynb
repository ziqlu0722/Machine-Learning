{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Multilayer-Perceptron.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziqlu0722/Machine-Learning/blob/master/Multilayer_Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "rG2EyE63LKsh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "log: Softmax with Cross Entropy Loss to be done"
      ]
    },
    {
      "metadata": {
        "id": "aClm6R33infR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import random\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KBOty8Xu6Fny",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1 Data Manipulation"
      ]
    },
    {
      "metadata": {
        "id": "QiaeuGTJ8sAa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Data Manipulation Functions**"
      ]
    },
    {
      "metadata": {
        "id": "_Ns2xAA0b3Oa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_data(link):\n",
        "    x = []\n",
        "    y = []\n",
        "    with open(link) as data:\n",
        "        for line in data:\n",
        "            y.append(line.strip().split(',')[0])\n",
        "            x.append(line.strip().split(',')[1:])\n",
        "    x = np.array(x).astype(np.float32)\n",
        "    y = np.array(y).astype(np.float32)\n",
        "    return x, y\n",
        "  \n",
        "def to_categorical(x):\n",
        "    \"\"\" One-hot encoding \"\"\"\n",
        "    n_col = np.max(x).astype(int) + 1\n",
        "    one_hot = np.zeros((x.shape[0], n_col))\n",
        "    one_hot[np.arange(x.shape[0]).astype(int), x.astype(int)] = 1 \n",
        "    return one_hot\n",
        "  \n",
        "def normalize(X, axis=-1, order=2):\n",
        "    \"\"\" normalize input data\"\"\"\n",
        "    l2 = np.atleast_1d(np.linalg.norm(X, order, axis))\n",
        "    l2[l2 == 0] = 1\n",
        "    return X / np.expand_dims(l2, axis)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mCVKi7nRa32u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x, y = load_data('drive/Data/wine/train_wine.csv')\n",
        "y = to_categorical(y)\n",
        "x = normalize(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0RLIBjI7ceFK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c2c162af-f46c-45f0-ed55-a402efc6229f"
      },
      "cell_type": "code",
      "source": [
        "y[:5]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "kfwits0TkRFw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "681ba9b7-3041-4360-b0f1-1952b6b5d622"
      },
      "cell_type": "code",
      "source": [
        "x.shape, y.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((151, 13), (151, 4))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "Y4045bGW6OBb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.2 MLP From Scratch"
      ]
    },
    {
      "metadata": {
        "id": "6SmnSnjs4nzb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###2.2.1 Loss Functions"
      ]
    },
    {
      "metadata": {
        "id": "YQrMDFynbTlj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Cross Entropy Loss**\n",
        "\n",
        "***1. Loss Function: ***\n",
        "\n",
        "[source](https://ml-cheatsheet.readthedocs.io/en/latest/loss_functions.html#mae-l1)\n",
        "\n",
        ">Cross-entropy loss, or log loss, measures the performance of a classification model whose output is a probability value between 0 and 1. Cross-entropy loss increases as the predicted probability diverges from the actual label. So predicting a probability of .012 when the actual observation label is 1 would be bad and result in a high loss value. A perfect model would have a log loss of 0.\n",
        "\n",
        ">The graph above shows the range of possible loss values given a true observation (isDog = 1). As the predicted probability approaches 1, log loss slowly decreases. As the predicted probability decreases, however, the log loss increases rapidly. Log loss penalizes both types of errors, but especially those predictions that are confident and wrong!\n",
        "\n",
        ">Cross-entropy and log loss are slightly different depending on context, but in machine learning when calculating error rates between 0 and 1 they resolve to the same thing.\n",
        "\n",
        "<img src=\"https://ml-cheatsheet.readthedocs.io/en/latest/_images/cross_entropy.png\" width=\"400\">\n",
        "\n",
        "\n",
        "$$L = -\\sum_{c=1}^ky_{i,c}\\ln~(\\sigma_{i, c})$$\n",
        "\n",
        "* $K$ - number of classes (dog, cat, fish)\n",
        "* $y$ - binary indicator (0 or 1) if class label $c$ is the correct classification for observation $i$\n",
        "* $p$ - predicted probability observation $i$ is of class $c$\n",
        "\n",
        "***2. Derivatives of Cross Entropy Loss: ***\n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L}{\\partial \\sigma_i} &= - \\sum_{c=1}^k y_k \\frac{\\partial ln~(\\sigma_k)}{\\partial \\sigma_k } \\\\\n",
        "&= - \\sum_{c=1}^k y_k \\frac{1}{\\sigma_k}\n",
        "\\end{align}\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "DdJvTCgObX0X",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Square Loss**\n",
        "\n",
        "*** 1.Loss Function***\n",
        "\n",
        "$$\\frac{(y-\\hat{y})^2}{2}$$\n",
        "    \n",
        "***2. Derivatives of Square Loss***\n",
        "\n",
        "$$-(y - \\hat{y})$$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "TMJSflk4M5CJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SquareLoss():\n",
        "  def __init__(self): pass\n",
        "\n",
        "  def loss(self, y, y_pred):\n",
        "      return 0.5 * np.power((y - y_pred), 2)\n",
        "  \n",
        "  def acc(self, y, p):\n",
        "    acc = np.count_nonzero(y[np.argmax(y, axis = 1) == np.argmax(p, axis = 1)])/len(y)\n",
        "    return acc\n",
        "\n",
        "  def gradient(self, y, y_pred):\n",
        "      return -(y - y_pred)\n",
        "    \n",
        "class cross_entropy_softmax():\n",
        "  def __init__(self): pass\n",
        "  \n",
        "  def loss(self, y, y_pred):\n",
        "    m = y.shape[0]\n",
        "    p = softmax(X)\n",
        "\n",
        "    log_likelihood = -np.log(p[range(m),y])\n",
        "    loss = np.sum(log_likelihood) / m\n",
        "    return loss\n",
        "  \n",
        "  def gradient(self, y, y_pred):\n",
        "    pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jhL05CTS55yu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###2.2.2 Activation Functions"
      ]
    },
    {
      "metadata": {
        "id": "KD0_OzOb9bin",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Softmax**\n",
        "\n",
        "\\begin{align}\\sigma_i = \\frac{e^{z_i}}{\\sum_k e^{z_k}}\\end{align}\n",
        "\n",
        "softmax of \n",
        "\n",
        "\\begin{align}\n",
        "\\left[\n",
        "\\begin{matrix}z_1, & z_2 & z_3, & ..., & z_k\\end{matrix}\n",
        "\\right]\\text{ > input of activation layer}\n",
        "\\end{align} is:\n",
        "\n",
        "\\begin{align}\n",
        "\\left[\\begin{matrix}\\frac{e^{z_1}}{\\sum_k e^{z_k}}, & \\frac{e^{z_2}}{\\sum_k e^{z_k}} & \\frac{e^{z_3}}{\\sum_k e^{z_k}}, & ..., & \\frac{e^{z_k}}{\\sum_k e^{z_k}}\\end{matrix}\n",
        "\\right]\\text{ > output of activation layer}\n",
        "\\end{align}\n",
        "\n",
        "**Derivative of Softmax**\n",
        "\n",
        "Jacobian Matrix Definition from [Wiki](https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant)\n",
        "\n",
        "vectorized:\n",
        "\n",
        "![Jacobian M](https://wikimedia.org/api/rest_v1/media/math/render/svg/74e93aa903c2695e45770030453eb77224104ee4)\n",
        "\n",
        "or element-wisely:\n",
        "\n",
        "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/f5be13db80a62a12665fad2f03414d1d445e9b10)\n",
        "\n",
        "We need to calculate the Jacobian Matrix for Softmax, which is:\n",
        "\n",
        "\\begin{align}\n",
        "\\sigma'_{(z_i)} =\n",
        "\\left[\n",
        "\\begin{matrix}\n",
        "{\\sigma_1}'_{(z_1)}, &  {\\sigma_1}'_{(z_2)}, & ... &{\\sigma_1}'_{(z_k)}\n",
        "\\\\{\\sigma_2}'_{(z_1)}, &  {\\sigma_2}'_{(z_2)}, & ... &{\\sigma_2}'_{(z_k)}\n",
        "\\\\{\\sigma_3}'_{(z_1)}, &  {\\sigma_3}'_{(z_2)}, & ... &{\\sigma_3}'_{(z_k)}\n",
        "\\\\...&...&...&...\n",
        "\\\\{\\sigma_k}'_{(z_1)}, &  {\\sigma_k}'_{(z_2),}& ... &{\\sigma_k}'_{(z_k)}\\\\\n",
        "\\end{matrix}\n",
        "\\right]\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "(1) if $i = j:$\n",
        "\n",
        "\\begin{align}\n",
        "{\\sigma_j}'_{(z_i)} &= (\\frac{e^{z_i}}{\\sum_k e^{z_k}})'_{(z_i)}\n",
        "\\\\&=\\frac{(e^{z_i})'_{(z_i)}(\\sum_ke^{z_k})-(e^{z_i})(\\sum_ke^{z_k})'_{(z_i)}}{(\\sum_k e^{z_k})^2}\n",
        "\\\\&=\\frac{(e^{z_i})(\\sum_ke^{z_k})-(e^{z_i})*1}{(\\sum_k e^{z_k})^2}\n",
        "\\\\&=\\frac{e^{z_k}}{\\sum_ke^{z_k}}(1-\\frac{1}{\\sum_k e^{z_k}})\n",
        "\\\\& = \\sigma_i(1-\\sigma_i)\n",
        "\\end{align}\n",
        "\n",
        "(2) if $i \\ne j:$\n",
        "\n",
        "\\begin{align}\n",
        "{\\sigma_j}'_{(z_i)} &= (\\frac{e^{z_j}}{\\sum_k e^{z_k}})'_{(z_i)}\n",
        "\\\\&=e^{z_j}*(-1)*\\frac{(e^{z_i})}{(\\sum_k e^{z_k})^2}\n",
        "\\\\& =-\\sigma_i\\sigma_j\n",
        "\\end{align}\n",
        "\n",
        "therefore,\n",
        "\n",
        "$$\n",
        "\\sigma'_{(z_i)} = \n",
        "\\left[\n",
        "         \\begin{matrix}\n",
        "\\sigma_1(1-\\sigma_1), &  \\sigma_1(0-\\sigma_2),& ... &  \\sigma_1(0-\\sigma_k)\\\\\n",
        "\\sigma_2(0-\\sigma_1), &  \\sigma_2(1-\\sigma_2),& ... &  \\sigma_2(0-\\sigma_k)\\\\\n",
        "\\sigma_3(0-\\sigma_1), &  \\sigma_3(0-\\sigma_2),& ... &  \\sigma_3(0-\\sigma_k)\\\\\n",
        "...&...&...&...\\\\\n",
        "\\sigma_k(0-\\sigma_1) &  \\sigma_k(0-\\sigma_2)& ... &  \n",
        "\\sigma_k(1-\\sigma_k)\n",
        "         \\end{matrix}\n",
        "\\right]\n",
        "$$\n",
        "\n",
        "or $$\\sigma'_{(z_i)} =\\sigma_j(\\delta_{ij} - \\sigma_i)$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\delta_{ij}=\n",
        "\\begin{cases}\n",
        "1& \\text{i=j}\\\\\n",
        "0& \\text{i$\\ne$j}\n",
        "\\end{cases}$$\n",
        "\n",
        "**Derivative of Softmax with Cross Entropy Loss**"
      ]
    },
    {
      "metadata": {
        "id": "q5jMy9FlOA_m",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Sigmoid**\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1ZSX-N5o9Zf0R2PzeHCZHiTWdiqYstuMZ\" width=\"500\">\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "S'(z_i) &= (\\frac{1}{1+e^{-z_i}})'\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "**Derivatives of Sigmoid**\n",
        "\n",
        "\\begin{aligned}\n",
        "S'(z) &= (\\frac{1}{1+e^{-z}})' \n",
        "\\\\\n",
        "&= \\frac{e^{-z}}{(1+e^{-z})^{2}} \n",
        "\\\\\n",
        "&= \\frac{1+e^{-z}-1}{(1+e^{-z})^{2}}  \n",
        "\\\\\n",
        "&= \\frac{1}{(1+e^{-z})} - \\frac{1}{(1+e^{-z})^2}\n",
        "\\\\\n",
        "&= \\frac{1}{(1+e^{-z})}(1-\\frac{1}{(1+e^{-z})}) \n",
        "\\\\\n",
        "&= S(z)(1-S(z))\n",
        "\\\\\n",
        "\\end{aligned}"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Ztv1V1flbIhj"
      },
      "cell_type": "markdown",
      "source": [
        "**Tanh**\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=14c37aZtyjsNYt_JcAZsA38UxNHBw8tf8\" width=\"500\">"
      ]
    },
    {
      "metadata": {
        "id": "sim0CVQXQIUL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Relu **\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=165FrzviKDSkbX8a5MxHW6ZcJk-VohEjV\" width=\"500\">"
      ]
    },
    {
      "metadata": {
        "id": "fvGrfeZCQZY2",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** Leaky Relu **\n",
        "\n",
        "<img src=\"http://drive.google.com/uc?export=view&id=1vn8D4CiAE4YvmP4p82K3qBlgHas-o4AG\" width=\"500\">"
      ]
    },
    {
      "metadata": {
        "id": "OTP_hoUsKqgs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Sigmoid():\n",
        "    def __call__(self, x):\n",
        "        return 1 / (1 + np.exp(-x))\n",
        "\n",
        "    def gradient(self, x):\n",
        "        return self.__call__(x) * (1 - self.__call__(x))\n",
        "      \n",
        "class Softmax():\n",
        "    def __call__(self, x):\n",
        "      \"\"\"-np.max(x)it shifts all of elements in the vector to negative to zero, \n",
        "      and negatives with large exponents saturate to zero rather than the infinity, \n",
        "      avoiding overflowing and resulting in nan\"\"\"\n",
        "      pass\n",
        "      \n",
        "class TanH():\n",
        "    def __call__(self, x):\n",
        "        return 2 / (1 + np.exp(-2*x)) - 1\n",
        "\n",
        "    def gradient(self, x):\n",
        "        return 1 - np.power(self.__call__(x), 2)\n",
        "\n",
        "class ReLU():\n",
        "    def __call__(self, x):\n",
        "        return np.where(x >= 0, x, 0)\n",
        "\n",
        "    def gradient(self, x):\n",
        "        return np.where(x >= 0, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4ELInYnX6W_G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###2.2.3 MLP Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "h96u1UJAhNcb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class MultilayerPerceptron():\n",
        "  \n",
        "  def __init__(self):\n",
        "      pass\n",
        "  \n",
        "  def _init_param(self, X, y, num_hid, acti_hid, acti_out, loss, epoch=3000, lr=0.01):\n",
        "      if acti_hid == 'relu':\n",
        "        self.hid_activation = ReLU()\n",
        "      elif acti_hid == 'sigmoid':\n",
        "        self.hid_activation = Sigmoid()\n",
        "      elif acti_hid == 'tanh':\n",
        "        self.hid_activation = TanH()\n",
        "      \n",
        "      if acti_out == 'sigmoid':\n",
        "        self.output_activation = Sigmoid()\n",
        "      elif acti_out == 'softmax':\n",
        "        self.output_activation = Softmax()\n",
        "        \n",
        "      if loss == 'cross_entropy':\n",
        "        self.loss = CrossEntropy()\n",
        "      elif loss == 'square_loss':\n",
        "        self.loss = SquareLoss()\n",
        "\n",
        "      self.epoch = epoch\n",
        "      self.lr = lr\n",
        "      self.num_hid = num_hid\n",
        "      self.num_sample, self.num_input = X.shape\n",
        "      self.num_output = y.shape[1]\n",
        "      # Hidden layer\n",
        "      self.w1 = np.random.uniform(size = (self.num_input, self.num_hid))\n",
        "      self.b1 = np.zeros(shape = (1, self.num_hid))\n",
        "      # Output layer\n",
        "      self.w2 = np.random.uniform(size = (self.num_hid, self.num_output))\n",
        "      self.b2 = np.zeros((1, self.num_output))\n",
        "      \n",
        "  def forwardpass(self, X):\n",
        "      self.hid_layer_input = X @ self.w1 + self.b1\n",
        "      self.hid_layer = self.hid_activation(self.hid_layer_input)\n",
        "      self.output_layer_input = self.hid_layer @ self.w2 + self.b2 \n",
        "      self.output_layer = self.output_activation(self.output_layer_input)   \n",
        "      return self.output_layer\n",
        "    \n",
        "  def backprop(self, X, y):\n",
        "      # grad. - loss to output layer input\n",
        "      print(self.loss.gradient(y, self.output_layer).shape)\n",
        "      print(self.output_activation.gradient(self.output_layer_input).shape)\n",
        "      grad_output = self.loss.gradient(y, self.output_layer) * self.output_activation.gradient(self.output_layer_input)\n",
        "      \n",
        "      grad_w2 = self.hid_layer.T @ grad_output\n",
        "      grad_b2 = np.sum(grad_output, axis=0, keepdims=True)\n",
        "      # grad. - hid layer_1 param\n",
        "      grad_hid_layer = grad_output.dot(self.w2.T) * self.hid_activation.gradient(self.hid_layer_input)\n",
        "      grad_w1 = X.T.dot(grad_hid_layer)\n",
        "      grad_b1 = np.sum(grad_hid_layer, axis=0, keepdims=True)\n",
        "\n",
        "      self.w2 -= self.lr * grad_w2\n",
        "      self.b2 -= self.lr * grad_b2\n",
        "      self.w1 -= self.lr * grad_w1\n",
        "      self.b1 -= self.lr * grad_b1\n",
        "\n",
        "  def batch_grad(self, X, y, num_hid, acti_hid, acti_out, loss, epoch=3000, lr=0.01):\n",
        "      self._init_param(X, y, num_hid, acti_hid, acti_out, loss, epoch=3000, lr=0.01)\n",
        "\n",
        "      for epoch in range(self.epoch):\n",
        "        y_pred = self.forwardpass(X)\n",
        "        self.backprop(X, y)\n",
        "\n",
        "        acc = self.loss.acc(y, y_pred)\n",
        "        loss = np.sum(self.loss.loss(y, y_pred))/len(y)\n",
        "            \n",
        "        if epoch % 100 == 0:\n",
        "          print('Epoch {} ---- Loss {:.4f} ---- Accuracy {:.4f}'.format(epoch+1, loss, acc))\n",
        "       \n",
        "  def predict(self, X):\n",
        "    \n",
        "      y_pred = self.fowardpass(X)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Fct5WMtWVsTi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "a3a359a1-4a01-4c78-b7c4-9f0de8136b85"
      },
      "cell_type": "code",
      "source": [
        "# Model_1\n",
        "\n",
        "num_hid = 20\n",
        "epoch = 100000\n",
        "lr = 0.001\n",
        "acti_hid = 'sigmoid'\n",
        "acti_out = 'sigmoid'\n",
        "loss = 'square_loss'\n",
        "\n",
        "mlp = MultilayerPerceptron()\n",
        "mlp.batch_grad(x, y, num_hid = num_hid, epoch = epoch, lr = lr, acti_hid = acti_hid, acti_out = acti_out, loss = loss) "
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 ---- Loss 1.4947 ---- Accuracy 0.0000\n",
            "Epoch 101 ---- Loss 0.8274 ---- Accuracy 0.0000\n",
            "Epoch 201 ---- Loss 0.3290 ---- Accuracy 0.3841\n",
            "Epoch 301 ---- Loss 0.3270 ---- Accuracy 0.3841\n",
            "Epoch 401 ---- Loss 0.3248 ---- Accuracy 0.3841\n",
            "Epoch 501 ---- Loss 0.3218 ---- Accuracy 0.3841\n",
            "Epoch 601 ---- Loss 0.3178 ---- Accuracy 0.4967\n",
            "Epoch 701 ---- Loss 0.3126 ---- Accuracy 0.6225\n",
            "Epoch 801 ---- Loss 0.3058 ---- Accuracy 0.6424\n",
            "Epoch 901 ---- Loss 0.2975 ---- Accuracy 0.6291\n",
            "Epoch 1001 ---- Loss 0.2881 ---- Accuracy 0.6225\n",
            "Epoch 1101 ---- Loss 0.2782 ---- Accuracy 0.6159\n",
            "Epoch 1201 ---- Loss 0.2687 ---- Accuracy 0.6093\n",
            "Epoch 1301 ---- Loss 0.2602 ---- Accuracy 0.6026\n",
            "Epoch 1401 ---- Loss 0.2529 ---- Accuracy 0.6026\n",
            "Epoch 1501 ---- Loss 0.2470 ---- Accuracy 0.6026\n",
            "Epoch 1601 ---- Loss 0.2422 ---- Accuracy 0.6026\n",
            "Epoch 1701 ---- Loss 0.2384 ---- Accuracy 0.6026\n",
            "Epoch 1801 ---- Loss 0.2353 ---- Accuracy 0.6026\n",
            "Epoch 1901 ---- Loss 0.2329 ---- Accuracy 0.6026\n",
            "Epoch 2001 ---- Loss 0.2309 ---- Accuracy 0.6093\n",
            "Epoch 2101 ---- Loss 0.2292 ---- Accuracy 0.6093\n",
            "Epoch 2201 ---- Loss 0.2279 ---- Accuracy 0.6093\n",
            "Epoch 2301 ---- Loss 0.2267 ---- Accuracy 0.6159\n",
            "Epoch 2401 ---- Loss 0.2258 ---- Accuracy 0.6159\n",
            "Epoch 2501 ---- Loss 0.2249 ---- Accuracy 0.6159\n",
            "Epoch 2601 ---- Loss 0.2242 ---- Accuracy 0.6159\n",
            "Epoch 2701 ---- Loss 0.2236 ---- Accuracy 0.6159\n",
            "Epoch 2801 ---- Loss 0.2230 ---- Accuracy 0.6159\n",
            "Epoch 2901 ---- Loss 0.2225 ---- Accuracy 0.6225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "QnMQwL8G6n8i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# # Model_2\n",
        "\n",
        "# num_hid = 24\n",
        "# epoch = 100000\n",
        "# lr = 0.0001\n",
        "# acti_hid = 'relu'\n",
        "# acti_out = 'softmax'\n",
        "# loss = 'cross_entropy'\n",
        "\n",
        "# mlp = MultilayerPerceptron()\n",
        "# mlp.batch_grad(x, y, num_hid = num_hid, epoch = epoch, lr = lr, acti_hid = acti_hid, acti_out = acti_out, loss = loss) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iok8HqXH6sJx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##2.3 MLP By Tensorflow"
      ]
    },
    {
      "metadata": {
        "id": "6g5PeDq74BNK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.1. Create Some Constants"
      ]
    },
    {
      "metadata": {
        "id": "cP2tb03-4BNL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "num_input = 13\n",
        "num_hid = 8\n",
        "num_output = 4\n",
        "lr = 0.01\n",
        "actf = tf.nn.sigmoid"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "e9PAgMVY4BNP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.2. Network Configuration"
      ]
    },
    {
      "metadata": {
        "id": "dDT9Te174BNP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " - Define layer with its weights and bias variables\n",
        " - and activation functions"
      ]
    },
    {
      "metadata": {
        "id": "O9BHSYiy4BNQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# create placeholder for x, y\n",
        "x_ = tf.placeholder(dtype = tf.float32, shape = [None, num_input])\n",
        "y_ = tf.placeholder(dtype = tf.float32, shape = [None, num_output])\n",
        "\n",
        "# create container for param variables\n",
        "hid_layer_param = {\n",
        "'w': tf.Variable(tf.random_normal([num_input, num_hid])),\n",
        "'b': tf.Variable(tf.zeros([num_hid]))}\n",
        "\n",
        "output_layer_param = {\n",
        "'w': tf.Variable(tf.random_normal([num_hid, num_output])),\n",
        "'b': tf.Variable(tf.zeros([num_output]))}\n",
        "\n",
        "hid_layer = actf(x_ @ hid_layer_param['w'] + hid_layer_param['b'])\n",
        "output_layer = actf(hid_layer @ output_layer_param['w'] + output_layer_param['b'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mANd-cZr4BNS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.3. Define Loss Function, Accuracy Calculation"
      ]
    },
    {
      "metadata": {
        "id": "xbWZnFB04BNT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "loss = tf.reduce_mean(tf.square(y - output_layer))\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(lr)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "correct_prediction = tf.equal(tf.argmax(y, axis = 1), tf.argmax(output_layer, axis = 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c7iqoCeb4BNV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.3.4. Training"
      ]
    },
    {
      "metadata": {
        "id": "g3QdW7M-4BNW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def MLP(num_epoch, x_test, y_test):\n",
        "    init = tf.global_variables_initializer()\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(init)\n",
        "\n",
        "        for epoch in range(num_epoch):\n",
        "            _, c, acc = sess.run([train, loss, accuracy], \n",
        "                                 feed_dict = {x_: x, y_: y})\n",
        "                       \n",
        "            if epoch % 100 == 0:\n",
        "              print('epoch {} - loss: {:0.4f} - acc: {:0.4f}'.format(epoch, c, acc))\n",
        "    \n",
        "    return hid_layer_param, output_layer_param"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uWsuyZ3k4BNZ",
        "colab_type": "code",
        "outputId": "2a2bbde2-0bf4-410e-b2c1-86686f581f18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 935
        }
      },
      "cell_type": "code",
      "source": [
        "num_epoch = 5000\n",
        "MLP(num_epoch, x_test, y_test)"
      ],
      "execution_count": 182,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 0 - loss: 0.2788 - acc: 0.3841\n",
            "epoch 100 - loss: 0.1625 - acc: 0.3841\n",
            "epoch 200 - loss: 0.1506 - acc: 0.6556\n",
            "epoch 300 - loss: 0.1325 - acc: 0.6225\n",
            "epoch 400 - loss: 0.1175 - acc: 0.6225\n",
            "epoch 500 - loss: 0.1092 - acc: 0.6225\n",
            "epoch 600 - loss: 0.1043 - acc: 0.6623\n",
            "epoch 700 - loss: 0.1000 - acc: 0.7152\n",
            "epoch 800 - loss: 0.0946 - acc: 0.7550\n",
            "epoch 900 - loss: 0.0872 - acc: 0.8079\n",
            "epoch 1000 - loss: 0.0788 - acc: 0.8675\n",
            "epoch 1100 - loss: 0.0705 - acc: 0.8874\n",
            "epoch 1200 - loss: 0.0631 - acc: 0.9139\n",
            "epoch 1300 - loss: 0.0568 - acc: 0.9205\n",
            "epoch 1400 - loss: 0.0515 - acc: 0.9272\n",
            "epoch 1500 - loss: 0.0473 - acc: 0.9272\n",
            "epoch 1600 - loss: 0.0437 - acc: 0.9272\n",
            "epoch 1700 - loss: 0.0408 - acc: 0.9338\n",
            "epoch 1800 - loss: 0.0383 - acc: 0.9338\n",
            "epoch 1900 - loss: 0.0362 - acc: 0.9338\n",
            "epoch 2000 - loss: 0.0345 - acc: 0.9404\n",
            "epoch 2100 - loss: 0.0329 - acc: 0.9470\n",
            "epoch 2200 - loss: 0.0315 - acc: 0.9470\n",
            "epoch 2300 - loss: 0.0303 - acc: 0.9470\n",
            "epoch 2400 - loss: 0.0292 - acc: 0.9536\n",
            "epoch 2500 - loss: 0.0282 - acc: 0.9536\n",
            "epoch 2600 - loss: 0.0273 - acc: 0.9536\n",
            "epoch 2700 - loss: 0.0265 - acc: 0.9536\n",
            "epoch 2800 - loss: 0.0257 - acc: 0.9536\n",
            "epoch 2900 - loss: 0.0250 - acc: 0.9536\n",
            "epoch 3000 - loss: 0.0243 - acc: 0.9536\n",
            "epoch 3100 - loss: 0.0236 - acc: 0.9536\n",
            "epoch 3200 - loss: 0.0230 - acc: 0.9536\n",
            "epoch 3300 - loss: 0.0224 - acc: 0.9536\n",
            "epoch 3400 - loss: 0.0218 - acc: 0.9536\n",
            "epoch 3500 - loss: 0.0212 - acc: 0.9536\n",
            "epoch 3600 - loss: 0.0206 - acc: 0.9603\n",
            "epoch 3700 - loss: 0.0201 - acc: 0.9603\n",
            "epoch 3800 - loss: 0.0196 - acc: 0.9603\n",
            "epoch 3900 - loss: 0.0190 - acc: 0.9603\n",
            "epoch 4000 - loss: 0.0185 - acc: 0.9603\n",
            "epoch 4100 - loss: 0.0181 - acc: 0.9669\n",
            "epoch 4200 - loss: 0.0176 - acc: 0.9669\n",
            "epoch 4300 - loss: 0.0171 - acc: 0.9669\n",
            "epoch 4400 - loss: 0.0167 - acc: 0.9669\n",
            "epoch 4500 - loss: 0.0163 - acc: 0.9735\n",
            "epoch 4600 - loss: 0.0158 - acc: 0.9868\n",
            "epoch 4700 - loss: 0.0154 - acc: 0.9868\n",
            "epoch 4800 - loss: 0.0151 - acc: 0.9868\n",
            "epoch 4900 - loss: 0.0147 - acc: 0.9868\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'b': <tf.Variable 'Variable_13:0' shape=(8,) dtype=float32_ref>,\n",
              "  'w': <tf.Variable 'Variable_12:0' shape=(13, 8) dtype=float32_ref>},\n",
              " {'b': <tf.Variable 'Variable_15:0' shape=(4,) dtype=float32_ref>,\n",
              "  'w': <tf.Variable 'Variable_14:0' shape=(8, 4) dtype=float32_ref>})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 182
        }
      ]
    }
  ]
}