{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NaiveBayes.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziqlu0722/Machine-Learning/blob/master/NaiveBayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "sYTMk542yhuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# NaiveBayes"
      ]
    },
    {
      "metadata": {
        "id": "bFsFZ_BVAzKK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1. Concept\n",
        "\n",
        "* Assume: Feature Independence\n",
        "* $P(x1, x2, ..., xd \\mid y) = P(x1\\mid y) * P(x2\\mid y) * ... * P(xd \\mid y)$\n",
        "\n",
        "This assumption doesn't really hold, but Naive Bayes still work in many cases, unless the assumption is completely broken\n"
      ]
    },
    {
      "metadata": {
        "id": "RR2R6sz8Be1A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 1.2 How to Calculate the Simple Uni-dimentional Density Function?\n",
        "\n",
        "* #### Option 1: Model\n",
        "\n",
        "> Apply an imposed model, calculate the maximum likelihood parameters for the model\n",
        "* gaussian, bernoullim, binomial, exponential\n",
        "* mixture of distributions\n",
        "\n",
        "* #### Option 2: Histogram\n",
        "\n",
        "> Bucket/cluster/bin and count feature values in each bucket/cluster/bin, and convert counts into probability"
      ]
    },
    {
      "metadata": {
        "id": "26wrAyDiGxiA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.3 But there are some defects with Naive Bayes to be solved\n",
        "\n",
        "* ### Problem 1: constant feature\n",
        "\n",
        "> if $x_j$ is constant, then some estimates is unusable\n",
        "\n",
        "> solutions:\n",
        "  1. control the parameters\n",
        "  2. smoothing, convert the counts into probabilities\n",
        "  3. feature selection, exclude this feature\n",
        "\n",
        "* ### Problem 2: zero probability\n",
        "This situation is common for sparse features, e.g. document data\n",
        "\n",
        "* ### Solution: Two Ways of Smoothing:\n",
        "\n",
        "  * $M$: # of observations\n",
        "\n",
        "  * $N$: # of features\n",
        "\n",
        "  * $t_i$: # of observations for the ith feature\n",
        "\n",
        "  * $P(i)$: Original Probability = $t_)/M$\n",
        "\n",
        "  (1). Laplace Smoothing: $ (t_i + 1 \\epsilon) / (M + N \\epsilon)$\n",
        "  \n",
        "  (2). Background + Foreground: $ \\lambda * P(i) + (1- \\lambda) * Q(i)$\n",
        "\n",
        "> *$Q(i)$: Sumation of $t_i^j$ / $M_i^j$ over $j$ experiments (from prior knowledge or preivous experiments)"
      ]
    },
    {
      "metadata": {
        "id": "qzuBUA42HF5D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###1.4 Major Types of Naive\n",
        "\n",
        "* **Multinomial Naive Bayes:**\n",
        "\n",
        "> This is mostly used for document classification problem, i.e whether a document belongs to the category of sports, politics, technology etc. The features/predictors used by the classifier are the frequency of the words present in the document.\n",
        "\n",
        "* **Bernoulli Naive Bayes:**\n",
        "\n",
        "> This is similar to the multinomial naive bayes but the predictors are boolean variables. The parameters that we use to predict the class variable take up only values yes or no, for example if a word occurs in the text or not.\n",
        "\n",
        "* **Gaussian Naive Bayes:**\n",
        "\n",
        "> When the predictors take up a continuous value and are not discrete, we assume that these values are sampled from a gaussian distribution.\n",
        "\n",
        "> *from [here](https://towardsdatascience.com/naive-bayes-classifier-81d512f50a7c)*"
      ]
    },
    {
      "metadata": {
        "id": "2OfD5AOhalFC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##1. Import Data"
      ]
    },
    {
      "metadata": {
        "id": "5Sja77-raV7A",
        "colab_type": "code",
        "outputId": "5d1720bd-66e0-4e9f-c9b5-9fb718bb7e5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections\n",
        "\n",
        "data = []\n",
        "\n",
        "with open('drive/Data/spambase/spambase.txt') as file:\n",
        "  for line in file:\n",
        "    line_split = line.strip().split(',')\n",
        "    data.append([float(c) for c in line_split])\n",
        "#     data.append(line.strip().split(','))\n",
        "\n",
        "print('There are {} data instances'. format(len(data))) \n",
        "print(collections.Counter(np.array(data)[:,-1]))"
      ],
      "execution_count": 729,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 4601 data instances\n",
            "Counter({0.0: 2788, 1.0: 1813})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "psWUExSUatZr",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2. Help Functions"
      ]
    },
    {
      "metadata": {
        "id": "YOn0DETYCFEt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###2.1. K-Folds Cross Validation"
      ]
    },
    {
      "metadata": {
        "id": "rR1dQvjpaY90",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, folds=5):\n",
        "\n",
        "  dataset_split = []     \n",
        "  dataset_copy = list(dataset)\n",
        "  fold_size = int(len(dataset) / folds)\n",
        "  for i in range(folds):\n",
        "    fold = []\n",
        "    while len(fold) < fold_size:\n",
        "      index = randrange(len(dataset_copy))\n",
        "      fold.append(dataset_copy.pop(index))\n",
        "    dataset_split.append(fold)\n",
        "\n",
        "  idx = 0\n",
        "  test_set = []\n",
        "  train_set = []\n",
        "  for i in range(folds):\n",
        "    test_set.append(dataset_split[idx])\n",
        "    train_set_j = []\n",
        "    train_set_k = []\n",
        "    for j in dataset_split[:idx] + dataset_split[idx + 1:]:\n",
        "      for k in j:\n",
        "        train_set_k.append(k)\n",
        "    train_set.append(train_set_k)\n",
        "    idx += 1\n",
        "  \n",
        "  return train_set, test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mMglEv0Od4Au",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# split data\n",
        "\n",
        "num_folds = 10\n",
        "train_set, test_set = cross_validation_split(data, folds = num_folds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2TsPrKqEy2sH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2. Metrics"
      ]
    },
    {
      "metadata": {
        "id": "fXaW3whOywzg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Metric:\n",
        "  def __init__(self, predict, label):    \n",
        "    self.predict = predict\n",
        "    self.label = label\n",
        "    self.num_obs = len(label)\n",
        "  \n",
        "  def acc(self):\n",
        "    predict = [np.argmin(_) for _ in self.predict]\n",
        "    err = 0\n",
        "    for i in range(len(self.label)):\n",
        "      if predict[i] != self.label[i]:\n",
        "          err += 1\n",
        "    acc = err/len(self.label)\n",
        "    print('Accuracy---{}'.format(acc))\n",
        "    return acc "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b6dqjRTca4AA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3. Build Algorithm"
      ]
    },
    {
      "metadata": {
        "id": "rwD5CNB7By-Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###3.1. Bernoulli NaiveBayes"
      ]
    },
    {
      "metadata": {
        "id": "TeK1_ecvax_K",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class BernoulliNB:\n",
        "  \n",
        "  def __init__(self, alpha = 1.0):\n",
        "    self.alpha = alpha\n",
        "    \n",
        "  \n",
        "  def fit(self, train_data):\n",
        "    x = np.array(train_data)[:,:-1]\n",
        "    y = np.array(train_data)[:,-1]\n",
        "    num_samples = x.shape[0]\n",
        "    num_features = x.shape[1]\n",
        "    \n",
        "    # split data for each class:\n",
        "    self.input = {'c0': x[y==0], 'c1':x[y==1]}\n",
        "\n",
        "    # calculate the prior as the frequency of class over all data points\n",
        "    self.prior = {'c0': 1 - y.mean(), 'c1': y.mean()}\n",
        "    \n",
        "    # calculate the mean for each x set\n",
        "    self.mu = x.mean(axis = 0)\n",
        "    \n",
        "    # calculate the size for each x set\n",
        "    self.size = {'c0': x[y==0].shape[0], 'c1': x[y==1].shape[0]}\n",
        "    \n",
        "    # calculate probability of each word consider smoothing\n",
        "    ''' smoothed_prob = (count + 1 * alpha) / (total_count + total_count_features * alpha)'''\n",
        "    \n",
        "    self.log_smoothed_p = {'c0': np.log((np.abs((np.count_nonzero(self.input['c0'] >= self.mu, axis = 0))) + self.alpha*1) / (self.size['c0'] + self.alpha*num_features)), \\\n",
        "                           'c1': np.log((np.abs((np.count_nonzero(self.input['c1'] >= self.mu, axis = 0))) + self.alpha*1) / (self.size['c1'] + self.alpha*num_features))}\n",
        "    \n",
        "    return self\n",
        "    \n",
        "  def predict(self, test_data):\n",
        "    x_test = np.array(test_data)[:,:-1]\n",
        "    label = np.array(test_data)[:,-1]\n",
        "    prob_test = {'c0':np.where(x_test >= self.mu, self.log_smoothed_p['c0'], 1 - self.log_smoothed_p['c0']), \\\n",
        "              'c1':np.where(x_test >= self.mu, self.log_smoothed_p['c1'], 1 - self.log_smoothed_p['c1'])} \n",
        "    predict = []\n",
        "    for i in range(x_test.shape[0]):\n",
        "      marginal_prob_c0 = np.sum(prob_test['c0'][i]) + np.log(self.prior['c0']) \n",
        "      marginal_prob_c1 = np.sum(prob_test['c1'][i]) + np.log(self.prior['c1']) \n",
        "      predict.append([marginal_prob_c0, marginal_prob_c1])\n",
        "    \n",
        "    return predict\n",
        "\n",
        "      \n",
        "  def evaluate(self, test_data):\n",
        "    test_x = np.array(test_data)[:,:-1]\n",
        "    label = np.array(test_data)[:,-1]\n",
        "    predict = self.predict(test_data)\n",
        "    metric = Metric(predict, label)\n",
        "    return metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G8oIcfPkrFvp",
        "colab_type": "code",
        "outputId": "48cbbee1-43ef-429f-bb62-520bc4620005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# set hyper parameters\n",
        "alpha = 200\n",
        "\n",
        "# train\n",
        "for i in range(num_folds):\n",
        "  acc = BernoulliNB(alpha = alpha).fit(train_set[i]).evaluate(test_set[i]).acc()\n",
        "print('The Accuracy Over 10 folds Cross Validation is {}'.format(np.mean(acc)))"
      ],
      "execution_count": 1034,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy---0.8978260869565218\n",
            "Accuracy---0.8934782608695652\n",
            "Accuracy---0.9021739130434783\n",
            "Accuracy---0.8782608695652174\n",
            "Accuracy---0.8956521739130435\n",
            "Accuracy---0.8760869565217392\n",
            "Accuracy---0.8739130434782608\n",
            "Accuracy---0.9347826086956522\n",
            "Accuracy---0.9130434782608695\n",
            "Accuracy---0.9043478260869565\n",
            "The Accuracy Over 10 folds Cross Validation is 0.9043478260869565\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_TchA-a1yqqM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2. Gaussian NaiveBayes"
      ]
    },
    {
      "metadata": {
        "id": "-CQjYp85zUV4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GaussianNB:\n",
        "  \n",
        "  def __init__(self, epsilon = 1.0):\n",
        "    # Epsilon is for smoothing the variance\n",
        "    self.epsilon = epsilon\n",
        "    \n",
        "  def smoothing(self, std):\n",
        "    std_smoothed = np.where(std > self.epsilon, std, self.epsilon)\n",
        "    return std_smoothed\n",
        " \n",
        "  def fit(self, train_data):\n",
        "    x = np.array(train_data)[:,:-1]\n",
        "    y = np.array(train_data)[:,-1]\n",
        "    num_samples = x.shape[0]\n",
        "    num_features = x.shape[1]\n",
        "    self.c = ['c0','c1']\n",
        "    \n",
        "    # split data for each class:\n",
        "    self.input = {'c0': x[y==0], 'c1':x[y==1]}\n",
        "\n",
        "    # calculate the prior as the frequency of class over all data points\n",
        "    self.prior = [1-y.mean(), y.mean()]\n",
        "    \n",
        "    # calculate the mean for each x set\n",
        "    self.mu = x.mean(axis = 0)\n",
        "    \n",
        "    # calculate the gaussian parameters for each feature   \n",
        "    \n",
        "    self.gaussian_mean = [self.input['c0'].mean(axis = 0), \\\n",
        "                          self.input['c1'].mean(axis = 0)]\n",
        "    \n",
        "    self.gaussian_std = [self.smoothing(np.std(self.input['c1'], axis = 0)), \\\n",
        "                         self.smoothing(np.std(self.input['c1'], axis = 0))]\n",
        " \n",
        "    return self\n",
        "  \n",
        "  def _gaussian(self, x, mean, std):\n",
        "    p = np.abs(np.exp(-((x-mean)**2/(2*std**2)))/(np.sqrt(2*np.pi)*std))\n",
        "    log_p = np.log(p)\n",
        "    \n",
        "    return log_p\n",
        "    \n",
        "  def predict(self, test_data):\n",
        "    x_test = np.array(test_data)[:,:-1]\n",
        "    label = np.array(test_data)[:,-1] \n",
        "    \n",
        "    pred = []\n",
        "    for i in range(x_test.shape[0]):\n",
        "      pred.append(np.sum(np.array([[self._gaussian(x_test[i][j], mu[j], std[j]) \\\n",
        "                                    for mu, std in list(zip(self.gaussian_mean, self.gaussian_std))] \\\n",
        "                                    for j in range(x_test.shape[1])]), axis = 0) + np.array(self.prior))\n",
        "    return pred\n",
        "      \n",
        "  def evaluate(self, test_data):\n",
        "    test_x = np.array(test_data)[:,:-1]\n",
        "    label = np.array(test_data)[:,-1]\n",
        "    predict = self.predict(test_data)\n",
        "    metric = Metric(predict, label)\n",
        "    return metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8l3MCsaXbC-E",
        "colab_type": "code",
        "outputId": "13d0f482-da14-4bf0-b644-5ffb60990456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "cell_type": "code",
      "source": [
        "# set hyper parameters\n",
        "epsilon = 1\n",
        "\n",
        "# train\n",
        "for i in range(num_folds):\n",
        "  acc = GaussianNB(epsilon = epsilon).fit(train_set[i]).evaluate(test_set[i]).acc()\n",
        "print('The Accuracy Over 10 folds Cross Validation is {}'.format(np.mean(acc)))"
      ],
      "execution_count": 1022,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy---0.8282608695652174\n",
            "Accuracy---0.8043478260869565\n",
            "Accuracy---0.8369565217391305\n",
            "Accuracy---0.7913043478260869\n",
            "Accuracy---0.8326086956521739\n",
            "Accuracy---0.8347826086956521\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: RuntimeWarning: divide by zero encountered in log\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy---0.8065217391304348\n",
            "Accuracy---0.8282608695652174\n",
            "Accuracy---0.808695652173913\n",
            "Accuracy---0.8326086956521739\n",
            "The Accuracy Over 10 folds Cross Validation is 0.8326086956521739\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yWrAoTAK8xKv",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3. Multinomial NaiveBayes"
      ]
    },
    {
      "metadata": {
        "id": "9xLyXq8xRkN_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class MultinomialNB:\n",
        "  \n",
        "  def __init__(self, alpha, num_bins):\n",
        "    '''alpha is the param for Laplace Smoothing'''\n",
        "    self.alpha = alpha\n",
        "    self.num_bins = num_bins\n",
        "    \n",
        "  \n",
        "  def fit(self, train_data):\n",
        "    x = np.array(train_data)[:,:-1]\n",
        "    y = np.array(train_data)[:,-1]\n",
        "    num_samples = x.shape[0]\n",
        "    self.num_features = x.shape[1]\n",
        "    self.num_class = len(np.unique(y))\n",
        "    \n",
        "    # class\n",
        "    self.c = ['c0', 'c1']\n",
        "    \n",
        "    # split data for each class:\n",
        "    self.input = {'c0': x[y==0], 'c1':x[y==1]}\n",
        "\n",
        "    # calculate the prior as the frequency of class over all data points\n",
        "    self.prior = {'c0': 1 - y.mean(), 'c1': y.mean()}\n",
        "    \n",
        "    # calculate the size for each x set\n",
        "    self.size = {'c0': x[y==0].shape[0], 'c1': x[y==1].shape[0]}\n",
        "    \n",
        "    # calculate the split point for each feature\n",
        "    # add 1e-10 here to include the max value into the bins\n",
        "    self.mu = x.mean(axis = 0)\n",
        "    self.bins = (x.max(axis = 0) + 1e-10 - x.min(axis = 0) + 1)/self.num_bins\n",
        "    start = x.min(axis = 0).reshape(1, self.num_features)\n",
        "    llist = []\n",
        "    for i in range(self.num_bins):\n",
        "      split = np.array((i + 1) * self.bins).reshape(1, self.num_features)\n",
        "      llist.append(split)\n",
        "    self.threshold = np.r_[start, np.concatenate(llist)]\n",
        "    \n",
        "    # calculate log smoothed bin probability of each feature consider smoothing\n",
        "    ''' smoothed_prob = (count + 1 * alpha) / (total_count + total_count_features * alpha)'''\n",
        "    self.log_prob = {}\n",
        "    for c in self.c:\n",
        "      prob = []\n",
        "      for i in range(self.num_bins):\n",
        "        match_mat = (self.input[c] >= self.threshold[i]) * (self.input[c] < self.threshold[i+1])\n",
        "        prob.append(np.log((np.count_nonzero(match_mat, axis = 0) + self.alpha*1)/(self.size[c] + self.alpha * self.num_features)))\n",
        "      self.log_prob[c] = np.array(prob)\n",
        "    \n",
        "    return self\n",
        "    \n",
        "  def predict(self, test_data):\n",
        "    x_test = np.array(test_data)[:,:-1]\n",
        "    label_test = np.array(test_data)[:,-1]\n",
        "    \n",
        "    prob = []\n",
        "    for i in range(self.num_bins):\n",
        "      match_mat = (x_test >= self.threshold[i]) * (x_test < self.threshold[i+1])  \n",
        "      prob.append(match_mat)  \n",
        "    # transpose here to make the iteration of instance at the uppermost layer\n",
        "    prob = np.array(prob).transpose(1,0,2)\n",
        "    \n",
        "    # get margainal prob for each instance\n",
        "    prob_test = {}\n",
        "    for c in self.c:     \n",
        "      prob_test[c] = np.array(prob * self.log_prob[c])\n",
        "         \n",
        "    predict = []\n",
        "    \n",
        "    for i in range(x_test.shape[0]):\n",
        "      marginal_prob_c0 = np.sum(prob_test['c0'][i]) + np.log(self.prior['c0']) \n",
        "      marginal_prob_c1 = np.sum(prob_test['c1'][i]) + np.log(self.prior['c1']) \n",
        "      predict.append([marginal_prob_c0, marginal_prob_c1])\n",
        "    \n",
        "    return predict\n",
        "      \n",
        "  def evaluate(self, test_data):\n",
        "    test_x = np.array(test_data)[:,:-1]\n",
        "    label = np.array(test_data)[:,-1]\n",
        "    metric = Metric(self.predict(test_data), label)\n",
        "    return metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XZ1JU3IudMT5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "6de1eda3-e302-45f4-f75d-b6f2cc57dea4"
      },
      "cell_type": "code",
      "source": [
        "# set hyper parameters\n",
        "alpha = 1\n",
        "num_bins = 4\n",
        "\n",
        "# train\n",
        "for i in range(num_folds):\n",
        "  acc = MultinomialNB(alpha = alpha, num_bins = num_bins).fit(train_set[i]).evaluate(test_set[i]).acc()\n",
        "print('The Accuracy Over 10 folds Cross Validation is {}'.format(np.mean(acc)))"
      ],
      "execution_count": 1009,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy---0.6543478260869565\n",
            "Accuracy---0.7021739130434783\n",
            "Accuracy---0.6739130434782609\n",
            "Accuracy---0.717391304347826\n",
            "Accuracy---0.7217391304347827\n",
            "Accuracy---0.7043478260869566\n",
            "Accuracy---0.691304347826087\n",
            "Accuracy---0.7108695652173913\n",
            "Accuracy---0.658695652173913\n",
            "Accuracy---0.7369565217391304\n",
            "The Accuracy Over 10 folds Cross Validation is 0.7369565217391304\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "LTvlTn-9dupS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "94752b90-71fe-40be-c2e8-607a4954e328"
      },
      "cell_type": "code",
      "source": [
        "# set hyper parameters\n",
        "alpha = 2\n",
        "num_bins = 9\n",
        "\n",
        "# train\n",
        "for i in range(num_folds):\n",
        "  acc = MultinomialNB(alpha = alpha, num_bins = num_bins).fit(train_set[i]).evaluate(test_set[i]).acc()\n",
        "print('The Accuracy Over 10 folds Cross Validation is {}'.format(np.mean(acc)))"
      ],
      "execution_count": 1010,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy---0.8239130434782609\n",
            "Accuracy---0.8021739130434783\n",
            "Accuracy---0.8217391304347826\n",
            "Accuracy---0.8478260869565217\n",
            "Accuracy---0.8326086956521739\n",
            "Accuracy---0.8304347826086956\n",
            "Accuracy---0.8130434782608695\n",
            "Accuracy---0.85\n",
            "Accuracy---0.8260869565217391\n",
            "Accuracy---0.8456521739130435\n",
            "The Accuracy Over 10 folds Cross Validation is 0.8456521739130435\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}