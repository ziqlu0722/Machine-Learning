{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GDA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ziqlu0722/Machine-Learning/blob/master/GDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "h7DtlXA3K33-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Generative Algorithms\n",
        "\n",
        "Gerative Models are density estimation.\n",
        "\n",
        "## Ultimate Task: Estimate $P(y\\mid x)$\n",
        "\n",
        "This Task of generative models is equivelent to: \n",
        "##Estimate $P(x \\mid y)$\n",
        "\n",
        "According to Bayes Rule: $$ P(x^1, x^2, x^3..., x^d \\mid y) = \\frac{P(y \\mid x^1, x^2, x^3..., x^d ) \\, P(y)}{P(x^1, x^2, x^3..., x^d )} $$\n",
        "\n",
        "*1.$ P(x^1, x^2, x^3..., x^d \\mid y)$  is the same for all y, not a factor in ranking $P(y\\mid x)$*\n",
        "\n",
        "*2.$ P(y)$ is prior, can be calculated from training counts if label is provided*\n",
        "\n",
        "___\n",
        "\n",
        "Below descriptions from [source](https://towardsdatascience.com/gaussian-discriminant-analysis-an-example-of-generative-learning-algorithms-2e336ba7aa5c) talkes about the difference between discriminative and generative algorithms\n",
        "\n",
        "## 1. Discriminative Learning Algorithms: \n",
        "\n",
        "In Linear Regression and Logistic Regression both we modelled conditional distribution of $y$ given $x$. Algorithms that model $P(x \\mid y)$ directly from the training set are called discriminative algorithms. \n",
        "\n",
        "## 2. Generative Learning Algorithms: \n",
        "\n",
        "There can be a different approach to the same problem, consider the same binary classification problem where we want learn to distinguish between two classes, class $A(y=1)$ and class $B (y=0)$ based on some features. Now we take all the examples of label $A$ and try to learn the features and build a model for class $A$. Then we take all the examples labeled $B$ and try to learn it’s features and build a separate model for class $B$. Finally to classify a new element, we match it against each model and see which one fits better (generate high value for probability). In this approach we try to model $P(x\\mid y)$  and $P(y)$ as oppose to $P(y)$ we did earlier, it’s called Generative Learning Algorithms. Once we learn the model $P(y)$ and $P(x\\mid y)$  using training set, we use Bayes Rule to derive the $P(y\\mid x)$. \n",
        "\n",
        "All in all, \n",
        "* generative approch >> model $P(x\\mid y)$ and $P(y)$ to estimate $P(y\\mid x)$ indirectly based on Bayes Rule\n",
        "* discriminative algorithm >> model $P(x\\mid y)$ directly\n",
        "\n",
        "## How to Calculate $P(x^1, x^2, ..., x^d\\mid y)?$\n",
        "\n",
        "- Option1: Assume Feature Independence - **Naive Bayes**\n",
        "\n",
        "- Option2: Model (restrict the joint distribution) - **GDA(gaussian)/EM(mixture)**\n",
        "  \n",
        "  - typically with a well known parameterized form\n",
        "  - estimate the parameters of the imposed model\n",
        "\n",
        "- Option3: Mix, Bend, Tweak Option 1 and 2 - **Bayesian Network/Graphical Model**\n",
        "  \n",
        "  - don't fully factorize by independence like Naive Bayes\n",
        "  - e.g. $P(x^1\\mid y) * P(x^2, x^3\\mid y) * P(x^4\\mid y)...$\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "V8VPoJgzK3mM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#Practice GDA\n",
        "\n",
        "### Problem\n",
        "\n",
        "Spam or Non-Spam ? - Binary Classification Problem"
      ]
    },
    {
      "metadata": {
        "id": "s7EuZVCDTnLm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step1: Load the Data"
      ]
    },
    {
      "metadata": {
        "id": "eAIOso9kbbxO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import collections"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Li7czMYTi3b",
        "colab_type": "code",
        "outputId": "065035d3-5766-4222-8a26-b91db73f8090",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "cell_type": "code",
      "source": [
        "data = []\n",
        "\n",
        "with open('drive/Machine-Learning/data/spambase/spambase.txt') as file:\n",
        "  for line in file:\n",
        "    line_split = line.strip().split(',')\n",
        "    data.append([float(c) for c in line_split])\n",
        "#     data.append(line.strip().split(','))\n",
        "\n",
        "print('There are {} data instances'. format(len(data))) \n",
        "print(collections.Counter(np.array(data)[:,-1]))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 4601 data instances\n",
            "Counter({0.0: 2788, 1.0: 1813})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5B1DNZaxdoXU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Step 2: K-Folds Cross Validation Help Function\n",
        "\n",
        "In K-Folds Cross Validation we split our data into k different subsets (or folds). We use k-1 subsets to train our data and leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then finalize our model. After that we test it against the test set. (cited from [source](https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6))\n",
        "\n",
        "![](https://cdn-images-1.medium.com/max/1600/1*J2B_bcbd1-s1kpWOu_FZrg.png)\n",
        "*Thanks to [Joseph Nelson](https://medium.com/@josephofiowa)  for this visual*"
      ]
    },
    {
      "metadata": {
        "id": "Hb4f6EIedndN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from random import randrange\n",
        "\n",
        "# Split a dataset into k folds\n",
        "def cross_validation_split(dataset, folds=5):\n",
        "\n",
        "  dataset_split = []     \n",
        "  dataset_copy = list(dataset)\n",
        "  fold_size = int(len(dataset) / folds)\n",
        "  for i in range(folds):\n",
        "    fold = []\n",
        "    while len(fold) < fold_size:\n",
        "      index = randrange(len(dataset_copy))\n",
        "      fold.append(dataset_copy.pop(index))\n",
        "    dataset_split.append(fold)\n",
        "\n",
        "  idx = 0\n",
        "  test_set = []\n",
        "  train_set = []\n",
        "  for i in range(folds):\n",
        "    test_set.append(dataset_split[idx])\n",
        "    train_set_j = []\n",
        "    train_set_k = []\n",
        "    for j in dataset_split[:idx] + dataset_split[idx + 1:]:\n",
        "      for k in j:\n",
        "        train_set_k.append(k)\n",
        "    train_set.append(train_set_k)\n",
        "    idx += 1\n",
        "  \n",
        "  return train_set, test_set"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w9UE59WunT4F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "###Step 3. GDA Algorithm Implementation"
      ]
    },
    {
      "metadata": {
        "id": "KsWCgPv_ncqk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**1. Maximum Likelihood**\n",
        "\n",
        "For supervised classification, we could model class conditional distribution of the data $P(X|y=k)$ for each class. Predictions can then be obtained by using Bayes’ rule: \n",
        "$$P(y=k | X) = \\frac{P(X | y=k) P(y=k)}{P(X)} = \\frac{P(X | y=k) P(y = k)}{ \\sum_{l} P(X | y=l) \\cdot P(y=l)}$$\n",
        "\n",
        "More specifically, for given data, we predict $P(y=k | X)$ by calculate, and compare the conditional probability in each class (right of the equation above), and then select the class which maximizes this conditional probability.\n",
        "\n",
        "$P(X | y=k)$ is derived from **Multivariate Gaussian distribution.**\n",
        "\n",
        "Multivariate Gaussian distribution, is parameterized by a mean vector $µ ∈ R^d$\n",
        "and a covariance matrix $Σ ∈ R^{d*d}$\n",
        ", where $Σ ≥ 0$ is symmetric and positive\n",
        "semi-definite. Also written $N (µ, Σ)$, its density is given by:\n",
        "\n",
        "$$P(X | y=k) = \\frac{1}{(2\\pi)^{d/2} |\\Sigma_k|^{1/2}}\\exp\\left(-\\frac{1}{2} (X-\\mu_k)^t \\Sigma_k^{-1} (X-\\mu_k)\\right)$$\n",
        "\n",
        "\n",
        "where $d$ is the number of features, \n",
        "$|\\Sigma_k|$ is the determinant of $\\Sigma$\n",
        "\n",
        "\n",
        "We can estimate from training data that:\n",
        "\n",
        "- $P(y=k)$ is: the proportion of instances of class\n",
        "\n",
        "- $\\mu_k$: empirical sample class means\n",
        "\n",
        "- $ \\Sigma_k$: either by the empirical sample class covariance matrices, or by a regularized estimator"
      ]
    },
    {
      "metadata": {
        "id": "0DgNsGa_y0Fu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "---\n",
        "In the case of LDA, the Gaussians for each class are assumed to share the same covariance matrix:  for all . This leads to linear decision surfaces, which can be seen by comparing the log-probability ratios: $\\log[P(y=k | X) / P(y=l | X)$]\n",
        "\n",
        "In the case of QDA, there are no assumptions on the covariance matrices  of the Gaussians, leading to quadratic decision surfaces.\n",
        "\n",
        "---\n",
        "*reference:* *[Andrew Ng](http://cs229.stanford.edu/notes/cs229-notes2.pdf)*, *[Scikit-learn Document](https://scikit-learn.org/stable/modules/lda_qda.html)*"
      ]
    },
    {
      "metadata": {
        "id": "eU-as3RstfAL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**2. Calculate Covariance Matrix:**\n",
        "\n",
        "$ X = \n",
        "  \\begin{bmatrix}\n",
        "    x_{11} &...x_{1j}...& x_{1d}  \\\\\n",
        "    ... & ... & ...\\\\\n",
        "    x_{i1} &...x_{ij}...& x_{id}  \\\\\n",
        "    ... & ... & ...\\\\\n",
        "    x_{n1}  &...x_{nj}...& x_{nd}  \\\\\n",
        "  \\end{bmatrix}$\n",
        "\n",
        "*Where $j\\in (0,d), i\\in (0, n)$*\n",
        "\n",
        "*$X: n \\times d$ matrix*\n",
        "\n",
        "$Cov(X)= N^{-1}X^TX \n",
        "= N^{-1} \\times \\begin{bmatrix}\n",
        "    x_{11} &...x_{i1}...& x_{n1}  \\\\\n",
        "    ... & ... & ...\\\\\n",
        "    x_{1j} & ...x_{ij}...& x_{ni}  \\\\\n",
        "    ... & ... & ...\\\\\n",
        "    x_{1d} &...x_{id}...&x_{nd}  \\\\\n",
        "  \\end{bmatrix} \\times \n",
        "  \\begin{bmatrix}\n",
        "    x_{11} &...x_{1j}...& x_{1d}  \\\\\n",
        "    ... & ... & ...\\\\\n",
        "    x_{i1} &...x_{ij}...& x_{id}  \\\\\n",
        "    ... & ... & ...\\\\\n",
        "    x_{n1}  &...x_{nj}...& x_{nd}  \\\\\n",
        "  \\end{bmatrix} \n",
        "=  N^{-1} \\times\n",
        "  \\begin{bmatrix}\n",
        "    \\sum_{i=1}^nΣ x_{i1}^2  &  ...\\sum_{i=1}^n x_{i1}x_{ij}. . .    &\t\\sum_{i=1}^n x_{i1}x_{id}\t \\\\\n",
        "        ... & ... & ...\\\\\n",
        "    \\sum_{i=1}^n x_{i1}x_{ij}  &  \t...\\sum_{i=1}^nΣ x_{ij}^2. . .    &\t\\sum_{i=1}^nx_2x_d \\\\\n",
        "        ... & ... & ...\\\\\n",
        "    \\sum_{i=1}^n x_{i1}x_{id}\t  &  \t...\\sum_{i=1}^nΣ x_{id}x_{ij}\t. . .    &\t\\sum_{i=1}^nx_{id}^2 \\\\\n",
        "  \\end{bmatrix}\n",
        "=  N^{-1} \\times\n",
        "  \\begin{bmatrix}\n",
        "    var(x_1)  &  ...cov(x_1,x_j). . .    &\tcov(x_1, x_d)\t \\\\\n",
        "        ... & ... & ...\\\\\n",
        "    cov(x_1, x_j)  &  \t...var(x_{ij}). . .    &\tcov(x_2, x_d) \\\\\n",
        "        ... & ... & ...\\\\\n",
        "    cov(x_1, x_d)\t  &  \t...cov(x_d, x_j)\t. . .    &\tvar(x_d) \\\\\n",
        "  \\end{bmatrix}$\n",
        "  \n",
        "  *$Cov(X): d \\times d$ matrix*\n"
      ]
    },
    {
      "metadata": {
        "id": "gcy1o_S2YYsE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Metric:\n",
        "  def __init__(self, predict, label):\n",
        "    self.predict = predict\n",
        "    self.label = label\n",
        "    self.num_obs = len(label)\n",
        "  \n",
        "  def acc(self):\n",
        "    err = 0\n",
        "    for i in range(len(self.label)):\n",
        "      if self.predict[i] != self.label[i]:\n",
        "          err += 1\n",
        "    acc = err/len(self.label)\n",
        "    print('Accuracy---{}'.format(acc))\n",
        "    return acc\n",
        "  \n",
        "#   def roc(self):\n",
        "#     true_positives = 0\n",
        "#     true_negatives = 0\n",
        "#     false_positives = 0\n",
        "#     false_negatives = 0\n",
        "\n",
        "#     for i in range(len(self.label)):\n",
        "#       if self.predict[i] == self.label[i] == 1:\n",
        "#           true_positives += 1\n",
        "#       elif self.predict[i] == self.label[i] == 0:\n",
        "#           true_negatives += 1\n",
        "#       elif self.predict[i] == 1 and self.label[i] == 0:\n",
        "#           false_positives += 1\n",
        "#       elif self.predict[i] == 0 and self.label[i] == 1:\n",
        "#           false_negatives += 1\n",
        "\n",
        "#     tpr =  (true_positives) / (true_positives + false_negatives)\n",
        "#     fpr = (false_positives) / (true_negatives + false_positives)\n",
        "#     return tpr, fpr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yqSKSq5erxkQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "class GDA:\n",
        "  \n",
        "  def __init__(self, data):\n",
        "    \"\"\"split the data set by classes\"\"\"\n",
        "    # split data into x and y\n",
        "    x = np.array(data)[:,:-1]\n",
        "    y = np.array(data)[:,-1]\n",
        "    \n",
        "    # split data for each class:\n",
        "    self.input = {'c0': x[y==0], 'c1':x[y==1]}\n",
        "\n",
        "    # calculate the prior as the frequency of class over all data points\n",
        "    self.prior = {'c0': 1 - y.mean(), 'c1': y.mean()}\n",
        "    \n",
        "    # calculate the mean for each x set:\n",
        "    self.mu = {'c0': x[y==0].mean(axis = 0), 'c1': x[y==1].mean(axis = 0)}\n",
        "    centered_x0 = np.mat(self.input['c0'] - self.mu['c0'])\n",
        "    centered_x1 = np.mat(self.input['c1'] - self.mu['c1'])\n",
        "    centered_x = np.mat(np.concatenate([centered_x0, centered_x1]))\n",
        "    \n",
        "    # calculate the sigma for each x set:\n",
        "    self.sigma = (1.0/len(y))*(centered_x.T @ centered_x)\n",
        "  \n",
        "  def predict(self, test_data):\n",
        "    test_x = np.array(test_data)[:,:-1]\n",
        "    predict = []\n",
        "    c0_pdf = multivariate_normal.pdf(test_x, self.mu['c0'], self.sigma)\n",
        "    c1_pdf = multivariate_normal.pdf(test_x, self.mu['c1'], self.sigma)\n",
        "    for i in range(len(test_data)):\n",
        "      if self.prior['c0'] * c0_pdf[i] < self.prior['c1'] * c1_pdf[i]:\n",
        "        predict.append(0)\n",
        "      else:\n",
        "        predict.append(1)\n",
        "    return predict\n",
        "  \n",
        "  def evaluate(self, test_data):\n",
        "    test_x = np.array(test_data)[:,:-1]\n",
        "    label = np.array(test_data)[:,-1]\n",
        "    predict = self.predict(test_data)\n",
        "    metric = Metric(predict, label)\n",
        "    return metric"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Kzpz_XZCB8pP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Step 4: Run the Algorithm with Cross-Validation **"
      ]
    },
    {
      "metadata": {
        "id": "1XxXGuMtZ9fp",
        "colab_type": "code",
        "outputId": "56c9e58b-4d22-41fb-d949-b938a97fb89a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "cell_type": "code",
      "source": [
        "# set hyper parameters\n",
        "num_folds = 10\n",
        "\n",
        "train_set, test_set = cross_validation_split(data, folds = num_folds)\n",
        "\n",
        "for i in range(num_folds):\n",
        "  acc = GDA(train_set[i]).evaluate(test_set[i]).acc()\n",
        "print('The Accuracy Over 10 folds Cross Validation is {}'.format(np.mean(acc)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy---0.9\n",
            "Accuracy---0.8717391304347826\n",
            "Accuracy---0.8956521739130435\n",
            "Accuracy---0.9108695652173913\n",
            "Accuracy---0.8913043478260869\n",
            "Accuracy---0.8673913043478261\n",
            "Accuracy---0.8673913043478261\n",
            "Accuracy---0.9021739130434783\n",
            "Accuracy---0.8934782608695652\n",
            "Accuracy---0.8695652173913043\n",
            "The Accuracy Over 10 folds Cross Validation is 0.8695652173913043\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}